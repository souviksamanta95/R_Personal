setwd("D:/IMI Docs/Insight/Blog/Logistic Regresion")
data <- read.csv("bank.csv")
head(data)
install.packages("InformationValue")
myData  <- read.csv ("bankloan.csv")   			# Read the file
def.df<-myData
nrow(def.df)
ncol(def.df)
head(def.df)
View(def.df)
def.df<- subset(myData, select = Obs:default)
nrow(def.df)
ncol(def.df)
head(def.df)
##Check Class bias
table(def.df$default)
# Create Training Data
input_ones  <- def.df[which(def.df$default == 1), ]  # all 1's
input_zeros <- def.df[which(def.df$default == 0), ]  # all 0's
set.seed(100)                                        # for repeatability of samples
input_ones_training_rows <- sample(1:nrow(input_ones), 0.8*nrow(input_ones))  # 1's for training
input_zeros_training_rows <- sample(1:nrow(input_zeros), 0.8*nrow(input_ones))  # 0's for training. Pick as many 0's as 1's
training_ones <- input_ones[input_ones_training_rows, ]
input_zeros_training_rows <- sample(1:nrow(input_zeros), 0.8*nrow(input_zeros))  # 0's for training. Pick as many 0's as 1's
training_ones <- input_ones[input_ones_training_rows, ]
input_ones_training_rows
View(input_ones)
input_ones_training_rows <- sample(1:nrow(input_ones), 0.8*nrow(input_ones))  # 1's for training
input_zeros_training_rows <- sample(1:nrow(input_zeros), 0.8*nrow(input_zeros))  # 0's for training. Pick as many 0's as 1's
training_ones <- input_ones[input_ones_training_rows, ]
training_zeros <- input_zeros[input_zeros_training_rows, ]
trainingData <- rbind(training_ones, training_zeros)  # row bind the 1's and 0's
table(trainingData$default)
# Create Test Data
test_ones <- input_ones[-input_ones_training_rows, ]
test_zeros <- input_zeros[-input_zeros_training_rows, ]
testData <- rbind(test_ones, test_zeros)  # row bind the 1's and 0's
table(testData$default)
#### Build Logit Models and Predict
options(scipen=99999)
logitMod <- glm(default ~ age + ed + employ + address + debtinc, data=trainingData, family=binomial(link="logit"))
####Model Diagnostics
summary(logitMod)
?glm
#Removing the outliers
data<- c(sample(x=1:20,size=40, replace=TRUE),65,80)
#Removing the outliers
data<- c(sample(x=1:20,size=40, replace=TRUE),65,80)
data
summary(data)
length(data)
data1<-(data)
length(data1)
boxplot(data1)
IQR(data1)
bench<- 15.75+1.5*IQR(data1) #interquartile range
bench
bench<- meidan(data1)+1.5*IQR(data1) #interquartile range
bench<- median(data1)+1.5*IQR(data1) #interquartile range
bench
bench<- q3(data1)+1.5*IQR(data1) #interquartile range
quarters(data1)
boxplot(data1)
summary(data)
length(data)
data1<-(data)
length(data1)
boxplot(data1)
bench<- 16+1.5*IQR(data1) #interquartile range
bench
data1<-data[data<31]
data1
summary(data1)
boxplot(data1)
#winsorizing the outliers
#replacing outliers with bench values
data
length(data)
summary(data)
boxplot(data)
bench<-16+1.5*IQR(data)
data[data>bench]
data[data>bench]=bench
summary(data)
boxplot(data)
length(data)
#variable transform
log_data<-log(data)
log_data
log_data
#treatment of missing values using regression
x<-1:10
y<-c(11,12,18,14,17,NA,NA,19,NA,27)
z<-c(19,11,2,14,20,4,9,10,18,1)
w<-c(1,4,7,10,3,5,7,6,6,9)
data<-data.frame(x,y,z,w)
data
#step 1: finding the most correlated value
cor(data)
cor(data, use="completed.obs")
#sysnum
symnum(cor(data,use="complete.obs"))
#which command : sees where the missing value is and where there is no
#missing value
#in whatever rows pair of x y w z , there is NA we have an indicator value I
#which will write 0 for presence of NA values and 1 for no NA values
#So, once we know this, we will only execute regression for rows which have 1
ind<- function(t)
{
x<- dim(length(t))
x[which(!is.na(t))]=1
x[which(is.na(t))]=0
return(x)
}
#step 1: finding the most correlated value
cor(data)
cor(data, use="completed.obs")
cor(data, use="complete.obs")
#sysnum
symnum(cor(data,use="complete.obs"))
#Step 3: Fitting the Linear model of y on x
lm(y~x,data=data)
summary(lm(y~x,data=data))
#y=9.743 +1.509*x
#Step 4: Imputation of missing values
for(i in 1:nrow(data))
{
if(data$I[i]==0)
{
data$y[i]=9.743 +1.509*data$x[i]
}
}
datasummary(data)
data$I<-ind(data$y)
data
setwd("C:/Users/souvi/Documents/R/BA/KNN")
lung <- read.csv("LungCapData2.csv")
attach(lung)
View(lung)
names(lung)
class(lung$Age)
library(ggplot2)
ggplot(lung, aes(x=LungCap, y=Age))+
geom_point()+geom_smooth()
cor(lung$Age,lung$LungCap)
mod <- lm(lung$LungCap ~ lung$Age)  # Always put the dependent variable inthe front
summary(mod)
attributes(mod)
#extracting the attrubutes
mod$coefficients
coef(mod)  #alternatively
#checking the plot of the model
plot(Age, LungCap, main = "Scatterplot")
abline(mod, col= "red", lwd =3)
#creating confidence level
confint(mod)
#charging the level of significance
confint(mod, level = 0.99)
#summary model
summary(mod)
#analysis of variance for the model
anova(mod)
getwd()
setwd("C:/Users/souvi/Documents/R/BA/Quiz")
data <- read.csv("Test.csv")
head(data)
View(data)
summary(data)
data
#step 1: finding the most correlated value
cor(data)
cor(data, use="complete.obs")
symnum(cor(data,use="complete.obs"))
ind<- function(t)
{
x<- dim(length(t))
x[which(!is.na(t))]=1
x[which(is.na(t))]=0
return(x)
}
ind
ind(data)
data2 <- ind(data)
data2
names(data)
setwd("C:/Users/souvi/Documents/R/BA/Quiz")
data <- read.csv("Test.csv")
head(data)
data <- read.csv("Test.csv")
head(data)
summary(data)
data
# Finding the most correlated value
cor(data)
cor(data, use="complete.obs")
symnum(cor(data,use="complete.obs"))     # Production of wheat is mostly related to year
# Getting the location of missing values---
ind<- function(t)
{
x<- dim(length(t))
x[which(!is.na(t))]=1
x[which(is.na(t))]=0
return(x)
}
data2 <- ind(data)
data2
names(data)
data$ <- ind(data)
data
data$Ind <- ind(data)
data
names(data)
data$Ind <- ind(data)
data <- read.csv("Test.csv")
head(data)
# Getting the location of missing values---
ind<- function(t)
{
x<- dim(length(t))
x[which(!is.na(t))]=1
x[which(is.na(t))]=0
return(x)
}
data$Ind <- ind(data)
data
data2 <- ind(data)
data$Ind <- data2
data$Ind <- ind(data$Production.of.wheat..MT.)
lm(Production.of.wheat..MT.~Year,data=data)
getwd()
setwd("C:/Users/souvi/Documents/R/BA/Quiz")
data <- read.csv("Test.csv")
head(data)
library(readxl)
data <- read_excel("Test.xlsx")
head(data)
summary(data)
data
# Finding the most correlated value
cor(data)
View(data)
cor(data, use="complete.obs")
symnum(cor(data,use="complete.obs"))     # Production of wheat is mostly related to year
library(readxl)
data <- read_excel("Test.xlsx")
head(data)
summary(data)
data
# Finding the most correlated value
cor(data)
cor(data, use="complete.obs")
summary(data)
data<- as.numeric(data)
# Finding the most correlated value
cor(data)
csv
setwd("C:/Users/souvi/Documents/R/BA/Quiz")
data <- read.csv("Test.csv")
head(data)
summary(data)
data
# Finding the most correlated value
cor(data)
cor(data, use="complete.obs")
colnames(data$ï..Year) <- "Year"
summary(data)
data
# Finding the most correlated value
cor(data)
cor(data, use="complete.obs")
symnum(cor(data,use="complete.obs"))     # Production of wheat is mostly related to year
# Getting the location of missing values---
ind<- function(t)
{
x<- dim(length(t))
x[which(!is.na(t))]=1
x[which(is.na(t))]=0
return(x)
}
data$Ind <- ind(data$Production.of.wheat..MT.)
data$Ind
names(data)
lm(Production.of.wheat..MT.~Year,data=data)
lm(Production.of.wheat..MT.~ï..Year,data=data)
summary(lm(Production.of.wheat..MT.~ï..Year,data=data)
lm(Production.of.wheat..MT.~ï..Year,data=data)
summary(lm(Production.of.wheat..MT.~ï..Year,data=data))
# y = -3107410 + 1588*x
# Imputation of missing values
for(i in 1:nrow(data))
{
if(data$I[i]==0)
{
data$y[i]=-3107410 + 1588*data$x[i]
}
}
datasummary(data)
summary(data)
summary(data)
data$Ind <- ind(data$Production.of.wheat..MT.)
data
summary(data)
# y = -3107410 + 1588*x
# Imputation of missing values
for(i in 1:nrow(data))
{
if(data$Ind[i]==0)
{
data$Production.of.wheat..MT.[i]=-3107410 + 1588*data$x[i]
}
}
summary(data)
# y = -3107410 + 1588*x
# Imputation of missing values
for(i in 1:nrow(data))
# y = -3107410 + 1588*x
# Imputation of missing values
for(i in 1:nrow(data))
{
if(data$Ind[i]==0)
{
data$Production.of.wheat..MT.[i]=-3107410 + 1588*data$ï..Year[i]
}
}
summary(data)
data$Ind <- ind(data$Production.of.wheat..MT.)
data
library(VIM)
data <- kNN(data)
names(data)
data <- subset(data, select = ï..Year:Quality.of.fertilizer)
head(data)
summary(data)
summary(lm(Production.of.wheat..MT.~.,data=data))
names(data)
summary(lm(Production.of.wheat..MT.~ï..Year,data=data))
summary(lm(Production.of.wheat..MT.~ï..Year,data=data))
summary(lm(Production.of.wheat..MT.~ï..Year,data=data))
summary(lm(Production.of.wheat..MT.~Amount.of.rainfall,data=data)) # R squared = 0.9824
summary(lm(Production.of.wheat..MT.~Qulaity.of.Soil,data=data))
summary(lm(Production.of.wheat..MT.~Quality.of.fertilizer,data=data)) # R squared =
# Question 2 :
summary(lm(Production.of.wheat..MT.~.,data=data))
# Multiple linear model ---
summary(lm(Production.of.wheat..MT.~.,data=data))
prod_2020 <- -3116000 + 1593*2020 - 1.743*585 - 322.9*6.5 - 37.23*7
prod_2020
getwd()
setwd("C:/Users/souvi/Documents/R/BA/Quiz")
data <- read.csv("Test.csv")
head(data)
summary(data)
data
getwd()
setwd("C:/Users/souvi/Documents/R/BA/Quiz")
data <- read.csv("Test.csv")
head(data)
# Finding the most correlated value
cor(data)
cor(data, use="complete.obs")
symnum(cor(data,use="complete.obs"))     # Production of wheat is mostly related to year
# Getting the location of missing values---
ind<- function(t)
{
x<- dim(length(t))
x[which(!is.na(t))]=1
x[which(is.na(t))]=0
return(x)
}
data$Ind <- ind(data$Production.of.wheat..MT.)
data$Ind
names(data)
lm(Production.of.wheat..MT.~ï..Year,data=data)
summary(lm(Production.of.wheat..MT.~ï..Year,data=data))
# y = -3107410 + 1588*x
# Imputation of missing values
for(i in 1:nrow(data))
{
if(data$Ind[i]==0)
{
data$Production.of.wheat..MT.[i]=-3107410 + 1588*data$ï..Year[i]
}
}
summary(data)
summary(data)
data$Ind <- ind(data$Production.of.wheat..MT.)
library(VIM)
data <- kNN(data)
data <- subset(data, select = ï..Year:Quality.of.fertilizer)
summary(data)
